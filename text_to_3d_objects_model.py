# -*- coding: utf-8 -*-
"""Text-to-3D-Objects-Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/bolleddu01234/text-to-3d-objects-model.82da347f-2661-47d7-b032-41ce7b1cbd30.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250408/auto/storage/goog4_request%26X-Goog-Date%3D20250408T141236Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D091c642b42a2dbaa457fc5a84619f33305fadde97d4bd3c353a4ef28f0e88ce23113733c0013d9d74edcbd4949070aed52a2c1618c56d87936f62350ade10b661eceb5c92453ce7358b62f8fb30b519a9982423fc6a48dca1faa0cd361b94b548da22a48e3b9fa2e9fa17b45cdf3c09a1d65597d5368b6a3cc692e3ee0d359bce5e454239a0ba727d43df6b9e5e682a770360e789ed8d51ef842dec1f34646ade3b7189d56d75845e16fd38bfa3974a08f25bd0c37644ba5febf3f1746a83667c8621d4c5ba0eec0acc722373b355cd0aff1f295d232d8be491e865c7fc45a5c21d27b03da2ec271ad9b5eb2c83b1e63eaecf63c8f6030cb77d94649d172bfec

<center> <h1>Text-to-3D Model</h1></center>
<br>
<center>Shap-E learns to generate 3D models by training an encoder to convert 3D objects into mathematical representations called implicit functions, then training a diffusion model to generate new 3D models based on those representations. The results are models with more-realistic textures and lighting effects â€“ and that are created more quickly </center> <br>
<center>
----
"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/openai/shap-e
# %cd shap-e
!pip install -e .

import torch
from shap_e.diffusion.sample import sample_latents
from shap_e.diffusion.gaussian_diffusion import diffusion_from_config
from shap_e.models.download import load_model, load_config
from shap_e.util.notebooks import create_pan_cameras, decode_latent_images, gif_widget
import warnings
warnings.filterwarnings("ignore", message="A NumPy version .* is required for this version of SciPy")

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

xm = load_model('transmitter', device=device)
model = load_model('text300M', device=device)
diffusion = diffusion_from_config(load_config('diffusion'))

# you can change render_mode_ to 'stf'or 'nerf'
# size = size of the renders --> higher values take longer to render

def generate_3d(prompt_, size_=64, render_mode_ ='nerf'):
    batch_size = 1
    guidance_scale = 15.0
    prompt = prompt_

    latents = sample_latents(
        batch_size=batch_size,
        model=model,
        diffusion=diffusion,
        guidance_scale=guidance_scale,
        model_kwargs=dict(texts=[prompt] * batch_size),
        progress=True,
        clip_denoised=True,
        use_fp16=True,
        use_karras=True,
        karras_steps=64,
        sigma_min=1e-3,
        sigma_max=160,
        s_churn=0)
    render_mode = render_mode_
    size = size_
    cameras = create_pan_cameras(size, device)
    for i, latent in enumerate(latents):
        images = decode_latent_images(xm, latent, cameras, rendering_mode=render_mode)
        display(gif_widget(images))

generate_3d('an airplane that looks like a banana', 256)

generate_3d('a chair that looks like an avocado')

generate_3d('a white teddy bear')

generate_3d('a bowl of fruits')

