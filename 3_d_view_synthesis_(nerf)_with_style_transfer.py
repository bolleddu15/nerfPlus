# -*- coding: utf-8 -*-
"""3-D View Synthesis (NeRF) with Style Transfer

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/bolleddu01234/3-d-view-synthesis-nerf-with-style-transfer.55df78ff-fbb7-47b5-804c-b635dff7a8d0.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250408/auto/storage/goog4_request%26X-Goog-Date%3D20250408T141417Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D234eca85f6039abe347311cfae9b885f286f2be61debbbfbb52a929fc8235def23ba9be1c52a85afd3fa830b6e2706a18b267ec06634e2295c20e498b44c44484832b9da13350fa7939d7e40864fee300518d06d34da06efaed4a1541a48dc54d1594f751056d0d2ed242ea29a8787b56f042d9a778b32a2949829ec59f3dd1bc3df6c7465aa771e32235a12c8f900e1f549a9cf84e64284ea898fbd067b44152e37184865c8faca6dbeab1313d836bee118ce524591eaea2a90fecada0386a38ad46cb632c702feb6027477e4e38702eb7624519a46e6e988833ab02a7c162197383612f589d6f24a93219c1d133453e587f244d156aff71a06c81699d1719a

## 3-D View Synthesis and Style Transfer over Objects

First of all we will be using Style transfer in order to apply the desired style on each and every image from the dataset then we will overfit the NeRF MLP to build the scene and check the original image vs the NeRF constructed image.

## Introduction to Style Transfer

Neural style transfer is an optimization technique used to take two images—a content image and a style reference image (such as an artwork by a famous painter) and blend them together so the output image looks like the content image, but “painted” in the style of the style reference image.
This is implemented by optimizing the output image to match the content statistics of the content image and the style statistics of the style reference image. These statistics are extracted from the images using a convolutional network.

Ex:

<img src='https://drive.google.com/uc?export=view&id=1haCHhofEqlP5xEy2Dev6w2ajHQ3qV7gt' width=800/>

## Introduction to NeRF

<img src='https://drive.google.com/uc?export=view&id=1j-9jcWTSfGXHWIqha_UL_CWmbqvsNMaC' width=750/>


The main purpose of this work is directly optimizing parameters of a continuous 5D scene representation
to minimize the error of rendering a set of captured images.
We represent a static scene as a continuous 5D function that outputs the
radiance emitted in each direction (θ, φ) at each point (x, y, z) in space, and a
density at each point which acts like a differential opacity controlling how much
radiance is accumulated by a ray passing through (x, y, z). Our method optimizes
a deep fully-connected neural network without any convolutional layers (often
referred to as a multilayer perceptron or MLP) to represent this function by
regressing from a single 5D coordinate (x, y, z, θ, φ) to a single volume density
and view-dependent RGB color.

So to wrap this up the neural network would hypothetically *memorize* (overfit on) the
image. This means that our neural network would have encoded the entire image
in its weights. We could query the neural network with each position,
and it would eventually reconstruct the entire image.

## Neural Radiance Field Scene Representation
The representation here is a continuous scene as a 5D vector-valued function whose input is
a 3D location x = (x, y, z) and 2D viewing direction (θ, φ), and whose output
is an emitted color c = (r, g, b) and volume density σ.

<img src='https://raw.githubusercontent.com/bmild/nerf/master/imgs/pipeline.jpg' width=800/>

In practice, we express direction as a 3D Cartesian unit vector d. We approximate this continuous 5D
scene representation with an MLP network F Θ : (x, d) → (c, σ) and optimize its
weights Θ to map from each input 5D coordinate to its corresponding volume
density and directional emitted color.
We encourage the representation to be multiview consistent by restricting
the network to predict the volume density σ as a function of only the location
x, while allowing the RGB color c to be predicted as a function of both location
and viewing direction. To accomplish this, the MLP F Θ first processes the input
3D coordinate x with 8 fully-connected layers (using ReLU activations and 256
channels per layer), and outputs σ and a 256-dimensional feature vector. This
feature vector is then concatenated with the camera ray’s viewing direction and
passed to one additional fully-connected layer (using a ReLU activation and 128
channels) that output the view-dependent RGB color.

### Setting up the environment
Importing the required dependencies and setting random seed and global variables to obtain reproducible results.
"""

!nvidia-smi

import os
import cv2
import glob
import imageio
import numpy as np
from tqdm import tqdm
import tensorflow as tf
import plotly.express as px
from datetime import datetime
import matplotlib.pyplot as plt

# Initialize global variables.
AUTO = tf.data.AUTOTUNE
BATCH_SIZE = 5
NUM_SAMPLES = 32
POS_ENCODE_DIMS = 16
EPOCHS = 250
tf.random.set_seed(42)

"""## Download and load the data
The `npz` data file contains images, camera poses, and a focal length.
The images are taken from multiple camera angles as shown in the introduction figure

To understand camera poses in this context we have to first allow
ourselves to think that a *camera is a mapping between the real-world
and the 2-D image*.
Consider the following equation:

<img src="https://i.imgur.com/TQHKx5v.pngg" width="100" height="50"/>

Where **x** is the 2-D image point, **X** is the 3-D world point and
**P** is the camera-matrix. **P** is a 3 x 4 matrix that plays the
crucial role of mapping the real world object onto an image plane.

<img src="https://i.imgur.com/chvJct5.png" width="300" height="100"/>

The camera-matrix is an *affine transform matrix* that is
concatenated with a 3 x 1 column `[image height, image width, focal length]`
to produce the *pose matrix*. This matrix is of
dimensions 3 x 5 where the first 3 x 3 block is in the camera’s point
of view. The axes are `[down, right, backwards]` or `[-y, x, z]`
where the camera is facing forwards `-z`.

![camera-mapping](https://i.imgur.com/kvjqbiO.png)

The COLMAP frame is `[right, down, forwards]` or `[x, -y, -z]`. Read
more about COLMAP [here](https://colmap.github.io/).
"""

# Downloading the data as it does not already exist.
file_name = "tiny_nerf_data.npz"
url = "https://people.eecs.berkeley.edu/~bmild/nerf/tiny_nerf_data.npz"
if not os.path.exists(file_name):
    data = tf.keras.utils.get_file(fname=file_name, origin=url)

data = np.load(data)
images = data["images"]
images_copy = images.copy()

im_shape = images.shape
(num_images, H, W, _) = images.shape
(poses, focal) = (data["poses"], data["focal"])

# Plot a random image from the dataset for visualization.
n = np.random.randint(low=0, high=num_images)
print('Images number is {} with shape {}x{} for eatch.'.format(num_images, W, H))
print('First Poses and Focal values')
print(poses[n])
print(focal)
plt.imshow(images[n])
plt.show()

"""## Loading Style Image"""

style = plt.imread('../input/nerfimg/styleimg.jpg')
content = plt.imread('../input/nerfimg/content.jpeg')
fig, axes = plt.subplots(1, 2, figsize=(10, 5))
axes[0].imshow(content)
axes[0].set_title('Content Image')
axes[1].imshow(style)
axes[1].set_title('Style Image')
plt.show()

"""## Understanding how a Neural Style Transfer works

If we want to code a Neural Style Transfer, the first thing we must do is to perfectly understand how convolutional neural networks work. In my case, I already talked about them in this post, although today it will be more specific.

In order for a Neural Style Transfer network to work, we must achieve at least two things:

* Convey the style as much as possible.
* Assure that the resulting image look as close to the original image as possible.

We could consider a third objective: making the resulting image as internally coherent as possible. This is something that Keras’s implementation includes but that, in my case, I am not going to dive into.

To do this, a neural style transfer network has the following:

* A convolutional neural network already trained (such as VGG19 or VGG16). Three images will be passed to this network: the base image, the style image, and the combination image. The latter could be both a noise image and the base image, although generally the base image is passed in order to make the resulting image look similar and to speed up the process.
* The combined image is optimized and steadily changes in such a way that it takes the styles of the style image while maintaining the content of the base image. To do this, we will optimize the image using two different losses (style and content).

<img src='https://cloudinary-res.cloudinary.com/image/upload/style_transfer_diagram.png' width=800/>


## How to transfer the style of an image

In convolutional neural networks, the deeper we go into the network, the more complex shapes the network distinguishes. This is something that can be clearly seen in the ConvNet Playground application, which allows you to see the layer channels at different “depths” of the network.

Therefore, if we want to transfer the style of an image, we will have to make the values of the features of the deep layers of our network look like those of the network of the style image.

But how can we calculate the loss function of this process in order to adjust it? For this, we use the so-called Gram Matrix.

## The Gram Matrix

Suppose we want to calculate the style loss on a layer. To do this, the first thing we must do is flatten our layer. This is a good thing, as the Gram Matrix calculation will not change based on the size of the layer.

<img src='https://sp-ao.shortpixel.ai/client/to_webp,q_glossy,ret_img,w_468/https://anderfernandez.com/wp-content/uploads/2020/10/Gram-Matrix.png' >

So, suppose we do a flatten on a layer of 3 filters. Well, the Gram Matrix shows the similarity between the filters and is obtained by calculating the dot product between the vectors:
When we are calculating dot products, we are taking the distance into account: the smaller the dot product, the closer the two vectors are and vice versa.
"""

def gram_matrix(x):
    x = tf.transpose(x, (2, 0, 1))
    features = tf.reshape(x, (tf.shape(x)[0], -1))
    gram = tf.matmul(features, tf.transpose(features))
    return gram

"""## Define Style and Content costs"""

def style_cost(style, combination):
    S = gram_matrix(style)
    C = gram_matrix(combination)
    channels = 3
    size = W * H
    return tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))

def content_cost(content, combination):
    return tf.reduce_sum(tf.square(combination - content))

"""## Loading the VGG19 Model"""

model = tf.keras.applications.vgg19.VGG19(weights='imagenet', include_top=False)
print(model.summary())

"""## Calculation of loss functions

Now that we have the model, we must create a function that extracts the values of that model for some given layers (in this way we can use it for both the content error and the style error).

To calculate the loss we will follow the following steps:

* Combine all the images in the same tensor.
* Get the values in all the layers for the three images. Yes, it is true that we will not need all the values of all the images, but this will be easier since we will already have everything extracted. In fact, if we wanted to change the extraction of styles, it would also be very simple.
* Initialize the loss vector where we will add the results.
* Extract the content layers for the base image and merge and calculate the content loss function.
* Extract the style layers for the style image and the combination image and calculate the style loss function.
"""

outputs_dict= dict([(layer.name, layer.output) for layer in model.layers])
feature_extractor = tf.keras.Model(inputs=model.inputs, outputs=outputs_dict)

cap_style = [
    "block1_conv1",
    "block2_conv1",
    "block3_conv1",
    "block4_conv1",
    "block5_conv1",
]

cap_content = "block5_conv2"

content_weight = 2.5e-8
style_weight = 1e-6

def loss_function(combination_image, base_image, style_reference_image):

    # 1. Combine all the images in the same tensioner.
    input_tensor = tf.concat(
        [base_image, style_reference_image, combination_image], axis=0
    )

    # 2. Get the values in all the layers for the three images.
    features = feature_extractor(input_tensor)

    #3. Inicializar the loss

    loss = tf.zeros(shape=())

    # 4. Extract the content layers + content loss
    layer_features = features[cap_content]
    base_image_features = layer_features[0, :, :, :]
    combination_features = layer_features[2, :, :, :]

    loss = loss + content_weight * content_cost(
        base_image_features, combination_features
    )
    # 5. Extract the style layers + style loss
    for layer_name in cap_style:
        layer_features = features[layer_name]
        style_reference_features = layer_features[1, :, :, :]
        combination_features = layer_features[2, :, :, :]
        sl = style_cost(style_reference_features, combination_features)
        loss += (style_weight / len(cap_style)) * sl

    return loss

@tf.function
def compute_loss_and_grads(combination_image, base_image, style_reference_image):
    with tf.GradientTape() as tape:
        loss = loss_function(combination_image, base_image, style_reference_image)
    grads = tape.gradient(loss, combination_image)
    return loss, grads

"""## Preprocess and deprocess images

The preprocessing of the images consists of giving the images the format that our network requires. In the case of Keras, as it is the VGG19 model, the model itself has an image preprocessing function: preprocess_input.

Keep in mind that Keras works with image batches. Therefore, the information we pass on must be in this format. To do this, carry out the following processes:

* load_image: we upload an image and give it a specific shape.
* img_to_array: we convert the loaded image into an array that considers the number of channels. In our case, being color images, we will have three channels, while a black and white image would have only one channel.
* expand_dims: we group all the images in a single array, since, as we have said, Keras works with batches of images. Thus, the result of this step will be an array with shape (3, width, height, 3).
* preprocess_input: subtract the mean of the RGB values from the Imagenet dataset (with which VGG19 is trained), in such a way that we get the images to have zero average. This is a typical preprocessing in images, as this prevents gradients from being too “extreme”, thus achieving better model results (link).
* convert_to_tensor: finally, we are going to convert our already centered array into a data type that Tensorflow understands. For that, we will simply convert it to a tensor with this function.
"""

def preprocess_image(image_path):
    # Util function to open, resize and format pictures into appropriate tensors
    img = cv2.resize(image_path, (W, H))
    img = tf.keras.preprocessing.image.img_to_array(img)
    img = np.expand_dims(img, axis=0)
    img = tf.keras.applications.vgg19.preprocess_input(img)
    return tf.convert_to_tensor(img)

"""## Deprocess the image"""

def deprocess_image(x):

    x = x.reshape((W, H, 3))

    x[:, :, 0] += 103.939
    x[:, :, 1] += 116.779
    x[:, :, 2] += 123.68

    x = x[:, :, ::-1]

    x = np.clip(x, 0, 255).astype("uint8")

    return x

# Commented out IPython magic to ensure Python compatibility.
# %%time
# img_nrows = W
# img_ncols = H
# optimizer = tf.keras.optimizers.Adam(
#     tf.keras.optimizers.schedules.ExponentialDecay(
#         initial_learning_rate=0.5, decay_steps=10, decay_rate=0.96
#     )
# )
# 
# base_image = preprocess_image(content)
# style_reference_image = preprocess_image(style*255)
# combination_image = tf.Variable(preprocess_image(content))
# 
# iterations = 50
# 
# for j in range(1, iterations + 1):
#     loss, grads = compute_loss_and_grads(
#         combination_image, base_image, style_reference_image
#     )
#     optimizer.apply_gradients([(grads, combination_image)])
#     if j % 10 == 0:
#         print("Iteration %d: loss=%.2f" % (j, loss))
# img = deprocess_image(combination_image.numpy())

plt.imshow(img)

"""## Network Design Output

Normally this design of style transfer performs well here is an example of the input and output for the network with the code <a href='https://github.com/EssamMohamedAbo-ElMkarem/Neural-Style-Transfer'><b>The Great Pyramids from the eyes of Vincent Van Gogh!!</b></a>

### Input:

<img src='https://drive.google.com/uc?export=view&id=1dbyKilLQxB7SKQoh7BVvb26H72jolW8m' width=750/>

### Output:

<img src='https://drive.google.com/uc?export=view&id=1Im6vfADF-QyqhcSn6T6TQS4glSIQ85RB' width=750/>

## Future Improvements

I have a plan to improve style transfer over NeRF's to be used as texture transfer which will provide a great representation for 3-D objects as a neural network and the weights can be updated depending on the textures we want to apply on specific parts of the image this will impact memory usage positively. A simpler representation for this provided here with a simple style transfer and then background removal. We can apply the prcess on each instance of the dataset or we can apply it on the spot but the first approach can be more safe as background removal can be tricky a little bit

## Volume Rendering with Radiance Fields

The 5D neural radiance field represents a scene as the volume density and di-
rectional emitted radiance at any point in space. We render the color of any ray
passing through the scene using principles from classical volume rendering.
The volume density σ(x) can be interpreted as the differential probability of a
ray terminating at an infinitesimal particle at location x. The expected color
C(r) of camera ray r(t) = o + td with near and far bounds t n and t f is:

<img src='https://drive.google.com/uc?export=view&id=1JFM-QlJFXa6l4WZ0FfGYHwqfKTygnNbz' />

The function T (t) denotes the accumulated transmittance along the ray from
t n to t, i.e., the probability that the ray travels from t n to t without hitting
any other particle. Rendering a view from our continuous neural radiance field
requires estimating this integral C(r) for a camera ray traced through each pixel
of the desired virtual camera.

So to have a little bit of an insight here we consider a ray, and we sample some random points on
the ray. These sample points each have a unique location `(x, y, z)`
and the ray has a viewing angle `(theta, phi)`. The viewing angle is
particularly interesting as we can shoot a ray through a single pixel
in a lot of different ways, each with a unique viewing angle. Another
interesting thing to notice here is the noise that is added to the
sampling process. We add a uniform noise to each sample so that the
samples correspond to a continuous distribution. In **The nextFigure** the
blue points are the evenly distributed samples and the white points
`(t1, t2, t3)` are randomly placed between the samples.

 ![img](https://i.imgur.com/r9TS2wv.gif)

Now to showcases the entire sampling process in 3D, where you
can see the rays coming out of the white image. This means that each
pixel will have its corresponding rays and each ray will be sampled at
distinct points.

 ![3-d rays](https://i.imgur.com/hr4D2g2.gif)

These sampled points act as the input to the NeRF model. The model is
then asked to predict the RGB color and the volume density at that
point.

<img src='https://drive.google.com/uc?export=view&id=1GIM9OXErQ7aStneQJekC5TILeRoxVY2o' />

## Positional Encoding
Despite the fact that neural networks are universal function approximators,
we found that having the network F Θ directly operate on xyzθφ input coordi-
nates results in renderings that perform poorly at representing high-frequency
variation in color and geometry. This is consistent with recent work by Rahaman
et al, which shows that deep networks are biased towards learning lower fre-
quency functions. They additionally show that mapping the inputs to a higher
dimensional space using high frequency functions before passing them to the
network enables better fitting of data that contains high frequency variation.
We leverage these findings in the context of neural scene representations, and
show that reformulating F Θ as a composition of two functions F Θ = F Θ 0 ◦ γ, one
learned and one not, significantly improves performance .
Here γ is a mapping from R into a higher dimensional space R 2L , and F Θ 0 is still
simply a regular MLP. Formally, the encoding function we use is:

<img src='https://drive.google.com/uc?export=view&id=1bko_N1R-gF-Z0Nx4MGNU0A6CS07SKp_J'/>

### Encoding the position into its corresponding feature.
"""

def encode_position(x):
    """Encodes the position into its corresponding Fourier feature.
    Args:
        x: The input coordinate.
    Returns:
        Fourier features tensors of the position.
    """
    positions = [x]
    for i in range(POS_ENCODE_DIMS):
        for fn in [tf.sin, tf.cos]:
            positions.append(fn(2.0**i * x))
    return tf.concat(positions, axis=-1)


def get_rays(height, width, focal, pose):
    """Computes origin point and direction vector of rays.
    Args:
        height: Height of the image.
        width: Width of the image.
        focal: The focal length between the images and the camera.
        pose: The pose matrix of the camera.
    Returns:
        Tuple of origin point and direction vector for rays.
    """
    # Build a meshgrid for the rays.
    i, j = tf.meshgrid(
        tf.range(width, dtype=tf.float32),
        tf.range(height, dtype=tf.float32),
        indexing="xy",
    )

    # Normalize the x axis coordinates.
    transformed_i = (i - width * 0.5) / focal

    # Normalize the y axis coordinates.
    transformed_j = (j - height * 0.5) / focal

    # Create the direction unit vectors.
    directions = tf.stack([transformed_i, -transformed_j, -tf.ones_like(i)], axis=-1)

    # Get the camera matrix.
    camera_matrix = pose[:3, :3]
    height_width_focal = pose[:3, -1]

    # Get origins and directions for the rays.
    transformed_dirs = directions[..., None, :]
    camera_dirs = transformed_dirs * camera_matrix
    ray_directions = tf.reduce_sum(camera_dirs, axis=-1)
    ray_origins = tf.broadcast_to(height_width_focal, tf.shape(ray_directions))

    # Return the origins and directions.
    return (ray_origins, ray_directions)

"""### Rendering the rays and flattening them"""

def render_flat_rays(ray_origins, ray_directions, near, far, num_samples, rand=False):
    """Renders the rays and flattens it.
    Args:
        ray_origins: The origin points for rays.
        ray_directions: The direction unit vectors for the rays.
        near: The near bound of the volumetric scene.
        far: The far bound of the volumetric scene.
        num_samples: Number of sample points in a ray.
        rand: Choice for randomising the sampling strategy.
    Returns:
       Tuple of flattened rays and sample points on each rays.
    """
    # Compute 3D query points.
    # Equation: r(t) = o+td -> Building the "t" here.
    t_vals = tf.linspace(near, far, num_samples)
    if rand:
        # Inject uniform noise into sample space to make the sampling
        # continuous.
        shape = list(ray_origins.shape[:-1]) + [num_samples]
        noise = tf.random.uniform(shape=shape) * (far - near) / num_samples
        t_vals = t_vals + noise

    # Equation: r(t) = o + td -> Building the "r" here.
    rays = ray_origins[..., None, :] + (
        ray_directions[..., None, :] * t_vals[..., None]
    )
    rays_flat = tf.reshape(rays, [-1, 3])
    rays_flat = encode_position(rays_flat)
    return (rays_flat, t_vals)

"""### Maping individual pose to flattened rays and sample points"""

def map_fn(pose):
    """Maps individual pose to flattened rays and sample points.
    Args:
        pose: The pose matrix of the camera.
    Returns:
        Tuple of flattened rays and sample points corresponding to the
        camera pose.
    """
    (ray_origins, ray_directions) = get_rays(height=H, width=W, focal=focal, pose=pose)
    (rays_flat, t_vals) = render_flat_rays(
        ray_origins=ray_origins,
        ray_directions=ray_directions,
        near=2.0,
        far=6.0,
        num_samples=NUM_SAMPLES,
        rand=True,
    )
    return (rays_flat, t_vals)

# Create the training split.
split_index = int(num_images * 0.8)

# Split the images into training and validation.
train_images = images[:split_index]
val_images = images[split_index:]

# Split the poses into training and validation.
train_poses = poses[:split_index]
val_poses = poses[split_index:]

# Make the training pipeline.
train_img_ds = tf.data.Dataset.from_tensor_slices(train_images)
train_pose_ds = tf.data.Dataset.from_tensor_slices(train_poses)
train_ray_ds = train_pose_ds.map(map_fn, num_parallel_calls=AUTO)
training_ds = tf.data.Dataset.zip((train_img_ds, train_ray_ds))
train_ds = (
    training_ds.shuffle(BATCH_SIZE)
    .batch(BATCH_SIZE, drop_remainder=True, num_parallel_calls=AUTO)
    .prefetch(AUTO)
)

# Making the validation pipeline.
val_img_ds = tf.data.Dataset.from_tensor_slices(val_images)
val_pose_ds = tf.data.Dataset.from_tensor_slices(val_poses)
val_ray_ds = val_pose_ds.map(map_fn, num_parallel_calls=AUTO)
validation_ds = tf.data.Dataset.zip((val_img_ds, val_ray_ds))
val_ds = (
    validation_ds.shuffle(BATCH_SIZE)
    .batch(BATCH_SIZE, drop_remainder=True, num_parallel_calls=AUTO)
    .prefetch(AUTO)
)

"""## NeRF model
The model is a multi-layer perceptron (MLP), with ReLU as its non-linearity.
An excerpt from the paper:
**"We encourage the representation to be multiview-consistent by
restricting the network to predict the volume density sigma as a
function of only the location `x`, while allowing the RGB color `c` to be
predicted as a function of both location and viewing direction. To
accomplish this, the MLP first processes the input 3D coordinate `x`
with 8 fully-connected layers (using ReLU activations and 256 channels
per layer), and outputs sigma and a 256-dimensional feature vector.
This feature vector is then concatenated with the camera ray's viewing
direction and passed to one additional fully-connected layer (using a
ReLU activation and 128 channels) that output the view-dependent RGB
color."**
Here we have gone for a minimal implementation and have used 128 units as mentioned in paper.
"""

def get_nerf_model(num_layers, num_pos):
    """Generates the NeRF neural network.
    Args:
        num_layers: The number of MLP layers.
        num_pos: The number of dimensions of positional encoding.
    Returns:
        The `tf.keras` model.
    """
    inputs = tf.keras.Input(shape=(num_pos, 2 * 3 * POS_ENCODE_DIMS + 3))
    x = inputs
    for i in range(num_layers):
        x = tf.keras.layers.Dense(units=128, activation="relu")(x)
        if i % 4 == 0 and i > 0:
            # Inject residual connection.
            x = tf.keras.layers.concatenate([x, inputs], axis=-1)
    outputs = tf.keras.layers.Dense(units=4)(x)
    return tf.keras.Model(inputs=inputs, outputs=outputs)

"""### Generating the RGB image and depth map from model prediction."""

def render_rgb_depth(model, rays_flat, t_vals, rand=True, train=True):
    """Generates the RGB image and depth map from model prediction.
    Args:
        model: The MLP model that is trained to predict the rgb and
            volume density of the volumetric scene.
        rays_flat: The flattened rays that serve as the input to
            the NeRF model.
        t_vals: The sample points for the rays.
        rand: Choice to randomise the sampling strategy.
        train: Whether the model is in the training or testing phase.
    Returns:
        Tuple of rgb image and depth map.
    """
    # Get the predictions from the nerf model and reshape it.
    if train:
        predictions = model(rays_flat)
    else:
        predictions = model.predict(rays_flat)
    predictions = tf.reshape(predictions, shape=(BATCH_SIZE, H, W, NUM_SAMPLES, 4))

    # Slice the predictions into rgb and sigma.
    rgb = tf.sigmoid(predictions[..., :-1])
    sigma_a = tf.nn.relu(predictions[..., -1])

    # Get the distance of adjacent intervals.
    delta = t_vals[..., 1:] - t_vals[..., :-1]
    # delta shape = (num_samples)
    if rand:
        delta = tf.concat(
            [delta, tf.broadcast_to([1e10], shape=(BATCH_SIZE, H, W, 1))], axis=-1
        )
        alpha = 1.0 - tf.exp(-sigma_a * delta)
    else:
        delta = tf.concat(
            [delta, tf.broadcast_to([1e10], shape=(BATCH_SIZE, 1))], axis=-1
        )
        alpha = 1.0 - tf.exp(-sigma_a * delta[:, None, None, :])

    # Get transmittance.
    exp_term = 1.0 - alpha
    epsilon = 1e-10
    transmittance = tf.math.cumprod(exp_term + epsilon, axis=-1, exclusive=True)
    weights = alpha * transmittance
    rgb = tf.reduce_sum(weights[..., None] * rgb, axis=-2)

    if rand:
        depth_map = tf.reduce_sum(weights * t_vals, axis=-1)
    else:
        depth_map = tf.reduce_sum(weights * t_vals[:, None, None], axis=-1)
    return (rgb, depth_map)

"""## Training
The training step is implemented as part of a custom `keras.Model` subclass
so that we can make use of the `model.fit` functionality.
"""

class NeRF(tf.keras.Model):
    def __init__(self, nerf_model):
        super().__init__()
        self.nerf_model = nerf_model

    def compile(self, optimizer, loss_fn):
        super().compile()
        self.optimizer = optimizer
        self.loss_fn = loss_fn
        self.loss_tracker = tf.keras.metrics.Mean(name="loss")
        self.psnr_metric = tf.keras.metrics.Mean(name="psnr")

    def train_step(self, inputs):
        # Get the images and the rays.
        (images, rays) = inputs
        (rays_flat, t_vals) = rays

        with tf.GradientTape() as tape:
            # Get the predictions from the model.
            rgb, _ = render_rgb_depth(
                model=self.nerf_model, rays_flat=rays_flat, t_vals=t_vals, rand=True
            )
            loss = self.loss_fn(images, rgb)

        # Get the trainable variables.
        trainable_variables = self.nerf_model.trainable_variables

        # Get the gradeints of the trainiable variables with respect to the loss.
        gradients = tape.gradient(loss, trainable_variables)

        # Apply the grads and optimize the model.
        self.optimizer.apply_gradients(zip(gradients, trainable_variables))

        # Get the PSNR of the reconstructed images and the source images.
        psnr = tf.image.psnr(images, rgb, max_val=1.0)

        # Compute our own metrics
        self.loss_tracker.update_state(loss)
        self.psnr_metric.update_state(psnr)
        return {"loss": self.loss_tracker.result(), "psnr": self.psnr_metric.result()}

    def test_step(self, inputs):
        # Get the images and the rays.
        (images, rays) = inputs
        (rays_flat, t_vals) = rays

        # Get the predictions from the model.
        rgb, _ = render_rgb_depth(
            model=self.nerf_model, rays_flat=rays_flat, t_vals=t_vals, rand=True
        )
        loss = self.loss_fn(images, rgb)

        # Get the PSNR of the reconstructed images and the source images.
        psnr = tf.image.psnr(images, rgb, max_val=1.0)

        # Compute our own metrics
        self.loss_tracker.update_state(loss)
        self.psnr_metric.update_state(psnr)
        return {"loss": self.loss_tracker.result(), "psnr": self.psnr_metric.result()}

    @property
    def metrics(self):
        return [self.loss_tracker, self.psnr_metric]

test_imgs, test_rays = next(iter(train_ds))
test_rays_flat, test_t_vals = test_rays
loss_list = []

class TrainMonitor(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        loss = logs["loss"]
        loss_list.append(loss)
        test_recons_images, depth_maps = render_rgb_depth(
            model=self.model.nerf_model,
            rays_flat=test_rays_flat,
            t_vals=test_t_vals,
            rand=True,
            train=False,
        )

        # Plot the rgb, depth and the loss plot.
        fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))
        ax[0].imshow(tf.keras.preprocessing.image.array_to_img(test_recons_images[0]))
        ax[0].set_title(f"Predicted Image: {epoch:03d}")

        ax[1].imshow(tf.keras.preprocessing.image.array_to_img(depth_maps[0, ..., None]))
        ax[1].set_title(f"Depth Map: {epoch:03d}")

        ax[2].plot(loss_list)
        ax[2].set_xticks(np.arange(0, EPOCHS + 1, 5.0))
        ax[2].set_title(f"Loss Plot: {epoch:03d}")

        fig.savefig(f"images/{epoch:03d}.png")
        plt.show()
        plt.close()

num_pos = H * W * NUM_SAMPLES
nerf_model = get_nerf_model(num_layers=8, num_pos=num_pos)

model = NeRF(nerf_model)
model.compile(
    optimizer=tf.keras.optimizers.Adam(), loss_fn=tf.keras.losses.MeanSquaredError()
)

# Create a directory to save the images during training.
if not os.path.exists("images"):
    os.makedirs("images")

model.fit(
    train_ds,
    validation_data=val_ds,
    batch_size=BATCH_SIZE,
    epochs=EPOCHS,
    callbacks=[TrainMonitor()],
    steps_per_epoch=split_index // BATCH_SIZE,
)

"""### Visualize the training step
Here we see the training step. With the decreasing loss, the rendered
image and the depth maps are getting better.
<img src= 'https://drive.google.com/uc?export=view&id=1J8_wVrjLse80bWIiovq0rvm0QtWBg21a'/>

"""

def create_gif(path_to_images, name_gif):
    filenames = glob.glob(path_to_images)
    filenames = sorted(filenames)
    images = []
    for filename in tqdm(filenames):
        images.append(imageio.imread(filename))
    kargs = {"duration": 0.25}
    imageio.mimsave(name_gif, images, "GIF", **kargs)

create_gif("images/*.png", "training.gif")

"""### Inference
In this section, we ask the model to build novel views of the scene.
The model was given `106` views of the scene in the training step. The
collections of training images cannot contain each and every angle of
the scene. A trained model can represent the entire 3-D scene with a
sparse set of training images.
Here we provide different poses to the model and ask for it to give us
the 2-D image corresponding to that camera view. If we infer the model
for all the 360-degree views, it should provide an overview of the
entire scenery from all around.
"""

nerf_model = model.nerf_model
test_recons_images, depth_maps = render_rgb_depth(
    model=nerf_model,
    rays_flat=test_rays_flat,
    t_vals=test_t_vals,
    rand=True,
    train=False,
)

fig, axes = plt.subplots(nrows=5, ncols=3, figsize=(10, 20))

for ax, ori_img, recons_img, depth_map in zip( axes, test_imgs,
                                              test_recons_images, depth_maps):
    ax[0].imshow(tf.keras.preprocessing.image.array_to_img(ori_img))
    ax[0].set_title("Original")

    ax[1].imshow(tf.keras.preprocessing.image.array_to_img(recons_img))
    ax[1].set_title("Reconstructed")

    ax[2].imshow(
        tf.keras.preprocessing.image.array_to_img(depth_map[..., None]), cmap="inferno"
    )
    ax[2].set_title("Depth Map")

"""## Saving the object as a model"""

model.save_weights('object.h5')

"""### Rendering 3D Scene
Here we will synthesize novel 3D views and stitch all of them together
to render a video encompassing the 360-degree view.
"""

def get_translation_t(t):
    """Get the translation matrix for movement in t."""
    matrix = [
        [1, 0, 0, 0],
        [0, 1, 0, 0],
        [0, 0, 1, t],
        [0, 0, 0, 1],
    ]
    return tf.convert_to_tensor(matrix, dtype=tf.float32)

def get_rotation_phi(phi):
    """Get the rotation matrix for movement in phi."""
    matrix = [
        [1, 0, 0, 0],
        [0, tf.cos(phi), -tf.sin(phi), 0],
        [0, tf.sin(phi), tf.cos(phi), 0],
        [0, 0, 0, 1],
    ]
    return tf.convert_to_tensor(matrix, dtype=tf.float32)

def get_rotation_theta(theta):
    """Get the rotation matrix for movement in theta."""
    matrix = [
        [tf.cos(theta), 0, -tf.sin(theta), 0],
        [0, 1, 0, 0],
        [tf.sin(theta), 0, tf.cos(theta), 0],
        [0, 0, 0, 1],
    ]
    return tf.convert_to_tensor(matrix, dtype=tf.float32)

def pose_spherical(theta, phi, t):
    """
    Get the camera to world matrix for the corresponding theta, phi
    and t.
    """
    c2w = get_translation_t(t)
    c2w = get_rotation_phi(phi / 180.0 * np.pi) @ c2w
    c2w = get_rotation_theta(theta / 180.0 * np.pi) @ c2w
    c2w = np.array([[-1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0, 1]]) @ c2w
    return c2w

rgb_frames = []
batch_flat = []
batch_t = []

# Iterate over different theta value and generate scenes.
for index, theta in tqdm(enumerate(np.linspace(0.0, 360.0, 120, endpoint=False))):
    # Get the camera to world matrix.
    c2w = pose_spherical(theta, -30.0, 4.0)


    ray_oris, ray_dirs = get_rays(H, W, focal, c2w)
    rays_flat, t_vals = render_flat_rays(
        ray_oris, ray_dirs, near=2.0, far=6.0, num_samples=NUM_SAMPLES, rand=False
    )

    if index % BATCH_SIZE == 0 and index > 0:
        batched_flat = tf.stack(batch_flat, axis=0)
        batch_flat = [rays_flat]

        batched_t = tf.stack(batch_t, axis=0)
        batch_t = [t_vals]

        rgb, _ = render_rgb_depth(
            nerf_model, batched_flat, batched_t, rand=False, train=False
        )

        temp_rgb = [np.clip(255 * img, 0.0, 255.0).astype(np.uint8) for img in rgb]
        rgb_frames = rgb_frames + temp_rgb
    else:
        batch_flat.append(rays_flat)
        batch_t.append(t_vals)

"""## Interactive Reconstructed RGB View"""

np_frames = np.asarray(rgb_frames)
fig = px.imshow(
    np_frames,
    animation_frame=0,
    binary_string=True,
    labels=dict(animation_frame="slice")
)
fig.update_layout(title="RGB Reconstruction")
fig.show()

!pip install imageio-ffmpeg
rgb_video = "rgb_video.mp4"
imageio.mimwrite(rgb_video, rgb_frames, fps=30, quality=9, macro_block_size=None)

"""### Visualize the video
Here we can see the rendered 360 degree view of the scene on the video inside the outputs folder. The model has successfully learned the entire volumetric space through the
sparse set of images in **only 250 epochs**.


"""

from ipywidgets import Video, Image
new_ = Video.from_file('rgb_video.mp4',play=True,width=200, height=200)
new_

"""## Paper Results

<img src='https://drive.google.com/uc?export=view&id=1WQHrrJBzHpkGFutwAwDNsdzDDcSX0qCV' />

Here is a comparisons on test-set views for scenes from the new synthetic dataset
generated with a physically-based renderer. the new method is able to recover fine
details in both geometry and appearance, such as Ship’s rigging, Lego’s gear
and treads, Microphone’s shiny stand and mesh grille, and Material ’s non-
Lambertian reflectance. LLFF exhibits banding artifacts on the Microphone
stand and Material ’s object edges and ghosting artifacts in Ship’s mast and
inside the Lego object. SRN produces blurry and distorted renderings in every
case. Neural Volumes cannot capture the details on the Microphone’s grille or
Lego’s gears, and it completely fails to recover the geometry of Ship’s rigging.

<img src='https://drive.google.com/uc?export=view&id=1CgejswKsGlMAs8Vs-LiZOCbHwR4sI18e' />


Here is a comparisons on test-set views of real world scenes. LLFF is specifically
designed for this use case (forward-facing captures of real scenes). the new method
is able to represent fine geometry more consistently across rendered views than
LLFF, as shown in Fern’s leaves and the skeleton ribs and railing in T-rex.
the new method also correctly reconstructs partially occluded regions that LLFF
struggles to render cleanly, such as the yellow shelves behind the leaves in the
bottom Fern crop and green leaves in the background of the bottom Orchid crop.
Blending between multiples renderings can also cause repeated edges in LLFF,
as seen in the top Orchid crop. SRN captures the low-frequency geometry and
color variation in each scene but is unable to reproduce any fine detail.

## Conclusion
There is no doubt that there has been a great deal of research on 3d imaging and all other computer vision applications that would enhance our lives in the very near future. That's why I've put an effort here trying to wrap the implementation provided by <b>Aritra Roy Gosthipaty</b> up with a quick explanation of the paper itself to put it as a milestone for anyone trying to dig into the topic from an applicable prespective. I've also modified the architecture of the neural network a little bit to improve the performance of the model. I really appretiate the great work done here in this paper by the researchers as actually the preliminary results are really promising and of course there has been many improvements in the field when it comes to NeRFs but this paper is considered a really good start to dig deeper into the topic.
###### Credits for the researchers behind this work
- Ben Mildenhall
- Pratul P. Srinivasan
- Matthew Tancik
- Jonathan T. Barron
- Ravi Ramamoorthi
- Ren Ng
- UC Berkeley
- Google Research
- UC San Diego
"""